import torch
import numpy as np
from tqdm import tqdm
from copy import deepcopy
import os

from . import embedding, distance, io_util
from .io_util import CMMDDataset


def perturb_state_dict(state_dict, scale_dict=None, default_scale=1e-4):
    perturbed_state_dict = {}
    for name, param in state_dict.items():
        if not torch.is_floating_point(param):
            perturbed_state_dict[name] = param
            continue
        scale = default_scale
        if scale_dict:
            for key, s in scale_dict.items():
                if key in name:
                    scale = s
                    break
        noise = torch.randn_like(param) * scale
        perturbed_state_dict[name] = param + noise
    return perturbed_state_dict


def generate_perturbed_models_conv(base_model, num_perturbs=5, scale_dict=None, perturb_scale=1e-3):
    """
    æ ¹æ® base_model ç”Ÿæˆå¤šä¸ªæ‰°åŠ¨æ¨¡å‹å‰¯æœ¬ã€‚
    """
    perturbed_models = []
    base_model = base_model._model
    model_config = base_model.config
    base_state_dict = deepcopy(base_model.state_dict())
    for _ in tqdm(range(num_perturbs), desc="ğŸ¨ ç”Ÿæˆæ‰°åŠ¨æ¨¡å‹"):
        perturbed_dict = perturb_state_dict(base_state_dict, scale_dict, perturb_scale)
        perturbed_model = base_model.__class__(model_config).eval()
        perturbed_model.load_state_dict(perturbed_dict, strict=False)
        if torch.cuda.is_available():
            perturbed_model = perturbed_model.cuda()
        perturbed_models.append(perturbed_model)
    return perturbed_models


def load_and_preprocess_image(img_path, input_size):
    """
    ä»è·¯å¾„åŠ è½½å›¾åƒï¼Œå¹¶ç¼©æ”¾åˆ°æ¨¡å‹æ‰€éœ€å°ºå¯¸ï¼Œè¿”å› shape=(1, H, W, 3)
    """
    dataset = CMMDDataset(img_path, reshape_to=input_size, is_single_file=True)
    image_np = dataset[0] / 255.0
    return np.expand_dims(image_np, axis=0)


def compute_cmmd_single_image_with_models(
    embedding_model,
    perturbed_models,
    single_img_path,
    ref_embs
):
    """
    ä½¿ç”¨å¤šä¸ªæ‰°åŠ¨æ¨¡å‹å¯¹å•å¼ å›¾ç‰‡è¿›è¡Œç‰¹å¾æå–ï¼Œå¹¶ä¸å‚è€ƒé›†è®¡ç®— MMD è·ç¦»ã€‚

    Args:
        embedding_model: ConvNeXtV2EmbeddingModel å®ä¾‹ã€‚
        perturbed_models: ç”± generate_perturbed_models_conv è¿”å›çš„æ‰°åŠ¨æ¨¡å‹åˆ—è¡¨ã€‚
        single_img_path: å•å¼ å›¾åƒè·¯å¾„ã€‚
        ref_embs: numpy.ndarrayï¼Œå‚è€ƒå›¾åƒåµŒå…¥ã€‚

    Returns:
        float: MMD è·ç¦»å€¼ã€‚
    """
    # âœ… åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ
    image_np = load_and_preprocess_image(single_img_path, embedding_model.input_image_size)

    # âœ… éå†æ‰°åŠ¨æ¨¡å‹å¹¶æå–è¯¥å›¾çš„å¤šç»„ç‰¹å¾
    single_embs = []
    for model in tqdm(perturbed_models, desc="ğŸ“¦ ä»convnextæ¨¡å‹æå–åµŒå…¥"):
        embedding_model._model = model
        with torch.no_grad():
            emb = embedding_model.embed(image_np).cpu().numpy()[0]
        single_embs.append(emb)

    single_embs = np.stack(single_embs, axis=0)

    # âœ… å¯¹é½å‚è€ƒé›†å¤§å°
    M = single_embs.shape[0]
    if ref_embs.shape[0] >= M:
        idx = np.random.choice(ref_embs.shape[0], size=M, replace=False)
        ref_embs_sampled = ref_embs[idx]
    else:
        N = ref_embs.shape[0]
        idx = np.random.choice(M, size=N, replace=False)
        single_embs = single_embs[idx]
        ref_embs_sampled = ref_embs

    return distance.mmd(ref_embs_sampled, single_embs).item()


def collect_single_image_from_each_subfolder(parent_dir):
    """
    ä»æ¯ä¸ªå­æ–‡ä»¶å¤¹ä¸­é€‰å–ç¬¬ä¸€å¼ å›¾ç‰‡ï¼Œè¿”å›å…¨è·¯å¾„åˆ—è¡¨ã€‚
    """
    valid_ext = (".png", ".jpg", ".jpeg")
    img_paths = []

    for subfolder in sorted(os.listdir(parent_dir)):
        subfolder_path = os.path.join(parent_dir, subfolder)
        if not os.path.isdir(subfolder_path):
            continue
        for file in sorted(os.listdir(subfolder_path)):
            if file.lower().endswith(valid_ext):
                img_paths.append(os.path.join(subfolder_path, file))
                break
    return img_paths
